{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "import pandas as pd\n",
    "from m3inference import M3Twitter\n",
    "from m3inference.utils import transform_jsonl_object_for_m3inference_text_model\n",
    "from googletrans import Translator\n",
    "TRANSLATOR = Translator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_jsonl('test.json')\n",
    "users = [d['user'] for d in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = []\n",
    "with open('test_processed_0.json', 'w') as f:\n",
    "    for tweet in data:\n",
    "        new_tweet = {}\n",
    "        new_tweet['lang'] = tweet['lang']\n",
    "        new_tweet['id_str'] = tweet['id_str']\n",
    "        new_tweet['text_original'] = clean_tweet_text(get_tweet_text(tweet))\n",
    "        new_user = {}\n",
    "        new_user['lang'] = tweet['user']['lang']\n",
    "        new_user['id'] = tweet['user']['id_str']\n",
    "        new_user['name'] = tweet['user']['name']\n",
    "        new_user['screen_name'] = tweet['user']['screen_name']\n",
    "        new_user['location_original'] = tweet['user']['location']\n",
    "        new_user['description_original'] = tweet['user']['description']\n",
    "  \n",
    "        new_tweet['user'] = new_user\n",
    "\n",
    "        new_data.append(new_tweet)\n",
    "        \n",
    "        f.write(json.dumps(new_tweet) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test_processed_1.json', 'w') as f:\n",
    "    for new_tweet in new_data:\n",
    "  \n",
    "        if new_tweet['lang'] != 'en':\n",
    "\n",
    "            text_translated = TRANSLATOR.translate(new_tweet['text_original'], dest='en')\n",
    "            new_tweet['text_english'] = text_translated.text  \n",
    "            new_tweet['text_lang_detected'] = text_translated.src  \n",
    "            \n",
    "        else: \n",
    "            new_tweet['text_english'] = new_tweet['text_original']\n",
    "            new_tweet['text_lang_detected'] = 'en'\n",
    "        \n",
    "        f.write(json.dumps(new_tweet) + '\\n')\n",
    "# 8m 35.7s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test_processed_2.json', 'w') as f:\n",
    "    for new_tweet in new_data:\n",
    "        \n",
    "        new_user = new_tweet['user']\n",
    "\n",
    "        if new_user['lang'] != 'en':\n",
    "            \n",
    "            location_translated = TRANSLATOR.translate(new_user['location_original'], dest='en')\n",
    "\n",
    "            new_user['location_english'] = location_translated.text\n",
    "            new_user['location_lang_detected'] = location_translated.src\n",
    "\n",
    "        else:\n",
    "            new_user['location_english'] = new_user['location_original']\n",
    "            new_user['location_lang_detected'] = 'en'\n",
    "            new_user['description_english'] = new_user['description_original']\n",
    "            new_user['description_lang_detected'] = 'en'\n",
    "        \n",
    "        f.write(json.dumps(new_tweet) + '\\n')\n",
    "# 11m 41.2s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test_processed_2.json', 'w') as f:\n",
    "    for new_tweet in new_data:\n",
    "        \n",
    "        new_user = new_tweet['user']\n",
    "\n",
    "        if new_user['lang'] != 'en':\n",
    "\n",
    "            description_translated = TRANSLATOR.translate(new_user['description_original'], dest='en')\n",
    "            new_user['description_english'] = description_translated.text\n",
    "            new_user['description_lang_detected'] = description_translated.src\n",
    "\n",
    "        \n",
    "        f.write(json.dumps(new_tweet) + '\\n')\n",
    "# 12m 35.2s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test_translated.json', 'w') as f:\n",
    "    for new_tweet in new_data:        \n",
    "        f.write(json.dumps(new_tweet) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "for new_tweet in new_data:\n",
    "    new_user = new_tweet[\"user\"]\n",
    "    new_user[\"text_lang_detected\"] = new_tweet[\"text_lang_detected\"]\n",
    "    new_user[\"text_lang\"] = new_tweet[\"lang\"]\n",
    "\n",
    "new_users = [new_tweet[\"user\"] for new_tweet in new_data]\n",
    "\n",
    "new_users_dict = {}\n",
    "for new_user in new_users:\n",
    "    user_id = new_user[\"id\"]\n",
    "    if user_id not in new_users_dict:\n",
    "        new_users_dict[user_id] = {}\n",
    "        for k, v in new_user.items():\n",
    "            if k != \"id\":\n",
    "                new_users_dict[user_id][k] = [v]\n",
    "            else:\n",
    "                new_users_dict[user_id][k] = v\n",
    "    else:\n",
    "        for k, v in new_user.items():\n",
    "            if v not in new_users_dict[user_id][k]:\n",
    "                new_users_dict[user_id][k].append(v)\n",
    "\n",
    "new_users = [new_users_dict[user_id] for user_id in new_users_dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'lang': [None],\n",
       "  'id': '1014342437936902146',\n",
       "  'name': ['WANARATâ™¥ï¸à¸§à¸­à¸£à¹Œ=à¸„à¸§à¸²à¸¡à¸ªà¸¸à¸‚ðŸ¤ŸðŸ»'],\n",
       "  'screen_name': ['PJSS06'],\n",
       "  'location_original': ['Thailand'],\n",
       "  'description_original': ['à¸§à¸­à¸£à¹Œà¸„à¸·à¸­à¸—à¸µà¹ˆ1à¹ƒà¸™à¹ƒà¸ˆ @warwanarat #warwanarat'],\n",
       "  'location_english': ['Thailand'],\n",
       "  'location_lang_detected': ['en'],\n",
       "  'description_english': ['War is number 1 in my heart @warwanarat #warwanarat'],\n",
       "  'description_lang_detected': ['th'],\n",
       "  'text_lang_detected': ['th', 'en'],\n",
       "  'text_lang': ['th', 'en']},\n",
       " {'lang': [None],\n",
       "  'id': '1175077409319612417',\n",
       "  'name': ['Rooster'],\n",
       "  'screen_name': ['mkamarul_'],\n",
       "  'location_original': ['Malaysia'],\n",
       "  'description_original': ['YNWAâ¤ï¸'],\n",
       "  'location_english': ['Malaysia'],\n",
       "  'location_lang_detected': ['en'],\n",
       "  'description_english': ['YNWAâ¤ï¸'],\n",
       "  'description_lang_detected': ['lus'],\n",
       "  'text_lang_detected': ['ms', 'en'],\n",
       "  'text_lang': ['in', 'en']}]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[user for user in new_users if len(user[\"text_lang_detected\"]) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_users = list(new_users_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[user for user in new_users if user['lang'][0] != None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/28/2023 04:36:02 - INFO - m3inference.m3inference -   Version 1.1.5\n",
      "08/28/2023 04:36:02 - INFO - m3inference.m3inference -   Running on cpu.\n",
      "08/28/2023 04:36:02 - INFO - m3inference.m3inference -   Will use text model. Note that as M3 was optimized to work well with both image and text data,                                     it is not recommended to use text only model unless you do not have the profile image.\n",
      "08/28/2023 04:36:03 - INFO - m3inference.m3inference -   Model text_model exists at C:\\Users\\An/m3/models/text_model.mdl.\n",
      "08/28/2023 04:36:03 - INFO - m3inference.utils -   Checking MD5 for model text_model at C:\\Users\\An/m3/models/text_model.mdl\n",
      "08/28/2023 04:36:03 - INFO - m3inference.utils -   MD5s match.\n",
      "08/28/2023 04:36:03 - INFO - m3inference.m3inference -   Loaded pretrained weight at C:\\Users\\An/m3/models/text_model.mdl\n"
     ]
    }
   ],
   "source": [
    "m3twitter=M3Twitter(cache_dir=\"./twitter_cache\", use_full_model=False, use_cuda=False, parallel=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
