{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_bz2_patterns = [\n",
    "    \"YYYY/mm/dd/HH/MM.json.bz2\",\n",
    "    \"YYYY/mm-b/dd/HH/MM.json.bz2\",\n",
    "    \"mm/dd/HH/MM.json.bz2\",\n",
    "    \"dd/HH/MM.json.bz2\",\n",
    "    \"YYYY/mm/dd/HH/MM.json.gz\",\n",
    "    \"YYYYmmdd/YYYYmmddHHMM00.json.gz\",\n",
    "]\n",
    "true_tar_patterns = [\n",
    "    \"twitter-json-scrape-YYYY-mm.zip.txt\",\n",
    "    \"archiveteam-twitter-YYYY-mm.tar.txt\",\n",
    "    \"archiveteam-twitter-stream-YYYY-mm.tar.txt\",\n",
    "    \"archiveteam-twitter-stream-YYYY-mm-b.tar.txt\",\n",
    "    \"twitter-stream-YYYY-mm-dd.tar.txt\",\n",
    "    \"twitter-YYYY-mm-dd.tar.txt\",\n",
    "    \"twitter_stream_YYYY_mm_dd.tar.txt\",\n",
    "    \"twitter-stream-YYYY-mm-dd.zip.txt\",\n",
    "    \"twitter-stream-YYYYmmdd.tar.txt\",\n",
    "]\n",
    "regex_bz2_patterns = [\n",
    "    r\"(\\d{4})/(\\d{2})/(\\d{2})/(\\d{2})/(\\d{2}).json.bz2\",\n",
    "    r\"(\\d{4})/(\\d{2})-b/(\\d{2})/(\\d{2})/(\\d{2}).json.bz2\",\n",
    "    r\"(\\d{2})/(\\d{2})/(\\d{2})/(\\d{2}).json.bz2\",\n",
    "    r\"(\\d{2})/(\\d{2})/(\\d{2}).json.bz2\",\n",
    "    r\"(\\d{4})/(\\d{2})/(\\d{2})/(\\d{2})/(\\d{2}).json.gz\",\n",
    "    r\"(\\d{4})(\\d{2})(\\d{2})/(\\d{4})(\\d{2})(\\d{2})(\\d{2})(\\d{2})00.json.gz\",\n",
    "]\n",
    "regex_tar_patterns = [\n",
    "    r\"twitter-json-scrape-(\\d{4})-(\\d{2}).zip.txt\",\n",
    "    r\"archiveteam-twitter-(\\d{4})-(\\d{2}).tar.txt\",\n",
    "    r\"archiveteam-twitter-stream-(\\d{4})-(\\d{2}).tar.txt\",\n",
    "    r\"archiveteam-twitter-stream-(\\d{4})-(\\d{2})-b.tar.txt\",\n",
    "    r\"twitter-stream-(\\d{4})-(\\d{2})-(\\d{2}).tar.txt\",\n",
    "    r\"twitter-(\\d{4})-(\\d{2})-(\\d{2}).tar.txt\",\n",
    "    r\"twitter_stream_(\\d{4})_(\\d{2})_(\\d{2}).tar.txt\",\n",
    "    r\"twitter-stream-(\\d{4})-(\\d{2})-(\\d{2}).zip.txt\",\n",
    "    r\"twitter-stream-(\\d{4})(\\d{2})(\\d{2}).tar.txt\",\n",
    "]\n",
    "\n",
    "regex_bz2_patterns_encrypt = {\n",
    "    r\"(\\d{4})/(\\d{2})/(\\d{2})/(\\d{2})/(\\d{2}).json.bz2\" : 1,\n",
    "    r\"(\\d{4})/(\\d{2})-b/(\\d{2})/(\\d{2})/(\\d{2}).json.bz2\" : 2,\n",
    "    r\"(\\d{2})/(\\d{2})/(\\d{2})/(\\d{2}).json.bz2\" : 3,\n",
    "    r\"(\\d{2})/(\\d{2})/(\\d{2}).json.bz2\" : 4,\n",
    "    r\"(\\d{4})/(\\d{2})/(\\d{2})/(\\d{2})/(\\d{2}).json.gz\" : 5,\n",
    "    r\"(\\d{4})(\\d{2})(\\d{2})/(\\d{4})(\\d{2})(\\d{2})(\\d{2})(\\d{2})00.json.gz\" : 6,\n",
    "}\n",
    "\n",
    "regex_tar_patterns_encrypt = {\n",
    "    r\"twitter-json-scrape-(\\d{4})-(\\d{2}).zip.txt\" : 1,\n",
    "    r\"archiveteam-twitter-(\\d{4})-(\\d{2}).tar.txt\" : 2,\n",
    "    r\"archiveteam-twitter-stream-(\\d{4})-(\\d{2}).tar.txt\" : 3,\n",
    "    r\"archiveteam-twitter-stream-(\\d{4})-(\\d{2})-b.tar.txt\" : 4,\n",
    "    r\"twitter-stream-(\\d{4})-(\\d{2})-(\\d{2}).tar.txt\" : 5,\n",
    "    r\"twitter-(\\d{4})-(\\d{2})-(\\d{2}).tar.txt\" : 6,\n",
    "    r\"twitter_stream_(\\d{4})_(\\d{2})_(\\d{2}).tar.txt\" : 7,\n",
    "    r\"twitter-stream-(\\d{4})-(\\d{2})-(\\d{2}).zip.txt\" : 8,\n",
    "    r\"twitter-stream-(\\d{4})(\\d{2})(\\d{2}).tar.txt\" : 9,\n",
    "}\n",
    "\n",
    "regex_bz2_patterns_decrypt_meaning = {\n",
    "    1: r\"(\\d{4})/(\\d{2})/(\\d{2})/(\\d{2})/(\\d{2}).json.bz2\",\n",
    "    2: r\"(\\d{4})/(\\d{2})-b/(\\d{2})/(\\d{2})/(\\d{2}).json.bz2\",\n",
    "    3: r\"(\\d{2})/(\\d{2})/(\\d{2})/(\\d{2}).json.bz2\",\n",
    "    4: r\"(\\d{2})/(\\d{2})/(\\d{2}).json.bz2\",\n",
    "    5: r\"(\\d{4})/(\\d{2})/(\\d{2})/(\\d{2})/(\\d{2}).json.gz\",\n",
    "    6: r\"(\\d{4})(\\d{2})(\\d{2})/(\\d{4})(\\d{2})(\\d{2})(\\d{2})(\\d{2})00.json.gz\",\n",
    "}\n",
    "\n",
    "regex_tar_patterns_decrypt_meaning = {\n",
    "    1: r\"twitter-json-scrape-(\\d{4})-(\\d{2}).zip.txt\",\n",
    "    2: r\"archiveteam-twitter-(\\d{4})-(\\d{2}).tar.txt\",\n",
    "    3: r\"archiveteam-twitter-stream-(\\d{4})-(\\d{2}).tar.txt\",\n",
    "    4: r\"archiveteam-twitter-stream-(\\d{4})-(\\d{2})-b.tar.txt\",\n",
    "    5: r\"twitter-stream-(\\d{4})-(\\d{2})-(\\d{2}).tar.txt\",\n",
    "    6: r\"twitter-(\\d{4})-(\\d{2})-(\\d{2}).tar.txt\",\n",
    "    7: r\"twitter_stream_(\\d{4})_(\\d{2})_(\\d{2}).tar.txt\",\n",
    "    8: r\"twitter-stream-(\\d{4})-(\\d{2})-(\\d{2}).zip.txt\",\n",
    "    9: r\"twitter-stream-(\\d{4})(\\d{2})(\\d{2}).tar.txt\",\n",
    "}\n",
    "\n",
    "regex_bz2_patterns_decrypt_str = {\n",
    "    1: r\"{}/{}/{}/{}/{}.json.bz2\",\n",
    "    2: r\"{}/{}-b/{}/{}/{}.json.bz2\",\n",
    "    3: r\"{}/{}/{}/{}.json.bz2\",\n",
    "    4: r\"{}/{}/{}.json.bz2\",\n",
    "    5: r\"{}/{}/{}/{}/{}.json.gz\",\n",
    "    6: r\"{}{}/{}{}00.json.gz\",\n",
    "}\n",
    "\n",
    "regex_tar_patterns_decrypt_str = {\n",
    "    1: r\"twitter-json-scrape-{}-{}.zip.txt\",\n",
    "    2: r\"archiveteam-twitter-{}-{}.tar.txt\",\n",
    "    3: r\"archiveteam-twitter-stream-{}-{}.tar.txt\",\n",
    "    4: r\"archiveteam-twitter-stream-{}-{}-b.tar.txt\",\n",
    "    5: r\"twitter-stream-{}-{}-{}.tar.txt\",\n",
    "    6: r\"twitter-{}-{}-{}.tar.txt\",\n",
    "    7: r\"twitter_stream_{}_{}_{}.tar.txt\",\n",
    "    8: r\"twitter-stream-{}-{}-{}.zip.txt\",\n",
    "    9: r\"twitter-stream-{}{}{}.tar.txt\",\n",
    "}\n",
    "\n",
    "with open(\"regex_bz2_patterns_codes.json\", \"w\") as f:\n",
    "    json.dump(regex_bz2_patterns_encrypt, f)\n",
    "\n",
    "with open(\"regex_tar_patterns_codes.json\", \"w\") as f:\n",
    "    json.dump(regex_tar_patterns_encrypt, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_pattern(input_string, regex_patterns):\n",
    "    \n",
    "    matched_pattern = None\n",
    "    extracted_values = None\n",
    "\n",
    "    for pattern in regex_patterns:\n",
    "        match = re.match(pattern, input_string)\n",
    "        if match:\n",
    "            matched_pattern = pattern\n",
    "            extracted_values = match.groups()\n",
    "            break\n",
    "\n",
    "    return matched_pattern, extracted_values\n",
    "\n",
    "def check_match(ymd_tuple, dhm_tuple):\n",
    "    if(ymd_tuple == None or dhm_tuple == None):\n",
    "        return True\n",
    "    ymd_tuple_full = list(ymd_tuple) + (5 - len(ymd_tuple)) * [None]\n",
    "    dhm_tuple_full = (5 - len(dhm_tuple)) * [None] + list(dhm_tuple)\n",
    "    for i in range(5):\n",
    "        if ymd_tuple_full[i] != dhm_tuple_full[i] and ymd_tuple_full[i] != None and dhm_tuple_full[i] != None:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "replace_patterns = {\n",
    "    \"{YYYY}/{}{}/{}{}/{}{}/{}{}.json.bz{}\" : \"YYYY/mm/dd/HH/MM.json.bz2\",\n",
    "    \"{YYYY}/{}{}-b/{}{}/{}{}/{}{}.json.bz{}\" : \"YYYY/mm-b/dd/HH/MM.json.bz2\",\n",
    "    \"{}{}/{}{}/{}{}/{}{}.json.bz{}\" : \"mm/dd/HH/MM.json.bz2\",\n",
    "    \"{}{}/{}{}/{}{}.json.bz{}\" : \"dd/HH/MM.json.bz2\",\n",
    "    \"\" : \"\",\n",
    "    \"{YYYY}/{}{}/{}{}/{}{}/{}{}.json.gz\" : \"YYYY/mm/dd/HH/MM.json.gz\",\n",
    "    \"{YYYY}{}{}{}{}/{YYYY}{}{}{}{}{}{}{}{}{}{}.json.gz\" : \"YYYYmmdd/YYYYmmddHHMM00.json.gz\",\n",
    "    \"{YYYY}{}{}{}{}/{YYYY}{}{}{}{}{}{}{YYYY}.json.gz\" : \"YYYYmmdd/YYYYmmddHHMM00.json.gz\",\n",
    "    \"{YYYY}{}{}{}{}/{YYYY}{}{}{}{}{}{YYYY}{}.json.gz\" : \"YYYYmmdd/YYYYmmddHHMM00.json.gz\",\n",
    "    \"{YYYY}{}{}{}{}/{YYYY}{}{}{}{}{YYYY}{}{}.json.gz\" : \"YYYYmmdd/YYYYmmddHHMM00.json.gz\",\n",
    "    \"{YYYY}{}{}{}{}/{YYYY}{}{}{}{YYYY}{}{}{}.json.gz\" : \"YYYYmmdd/YYYYmmddHHMM00.json.gz\",\n",
    "    \"{YYYY}{}{}{}{}/{YYYY}{}{}{YYYY}{}{}{}{}.json.gz\" : \"YYYYmmdd/YYYYmmddHHMM00.json.gz\",\n",
    "    \"{YYYY}{}{}{}{}/{YYYY}{}{}{YYYY}{YYYY}.json.gz\" : \"YYYYmmdd/YYYYmmddHHMM00.json.gz\",\n",
    "    \"{YYYY}{}{}{}{}/{YYYY}{}{YYYY}{}{}{}{}{}.json.gz\" : \"YYYYmmdd/YYYYmmddHHMM00.json.gz\",\n",
    "    \"{YYYY}{}{}{}{}/{YYYY}{}{YYYY}{}{YYYY}.json.gz\" : \"YYYYmmdd/YYYYmmddHHMM00.json.gz\",\n",
    "    \"{YYYY}{}{}{}{}/{YYYY}{}{YYYY}{YYYY}{}.json.gz\" : \"YYYYmmdd/YYYYmmddHHMM00.json.gz\",\n",
    "    'twitter-json-scrape-{YYYY}-{}{}.zip.txt': \"twitter-json-scrape-YYYY-mm.zip.txt\",\n",
    "    'archiveteam-twitter-{YYYY}-{}{}.tar.txt': \"archiveteam-twitter-YYYY-mm.tar.txt\",\n",
    "    'archiveteam-twitter-stream-{YYYY}-{}{}.tar.txt': \"archiveteam-twitter-stream-YYYY-mm.tar.txt\",\n",
    "    'archiveteam-twitter-stream-{YYYY}-{}{}-b.tar.txt': \"archiveteam-twitter-stream-YYYY-mm-b.tar.txt\",\n",
    "    'twitter-stream-{YYYY}-{}{}-{}{}.tar.txt': \"twitter-stream-YYYY-mm-dd.tar.txt\",\n",
    "    'twitter-{YYYY}-{}{}-{}{}.tar.txt': \"twitter-YYYY-mm-dd.tar.txt\",\n",
    "    'twitter_stream_{YYYY}_{}{}_{}{}.tar.txt': \"twitter_stream_YYYY_mm_dd.tar.txt\",\n",
    "    'twitter-stream-{YYYY}-{}{}-{}{}.zip.txt': \"twitter-stream-YYYY-mm-dd.zip.txt\",\n",
    "    'twitter-stream-{YYYY}{}{}{}{}.tar.txt': \"twitter-stream-YYYYmmdd.tar.txt\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.load(open('data_folder_structure.json', 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bz2_patterns = {}\n",
    "\n",
    "for year in data.keys():\n",
    "    bz2_patterns[year] = {}\n",
    "    for month in data[year].keys():\n",
    "        bz2_patterns[year][month] = {}\n",
    "        for tar_file in data[year][month]:\n",
    "            tar_pattern, tar_values = match_pattern(tar_file, regex_tar_patterns)\n",
    "            tar_pattern = regex_tar_patterns_encrypt[tar_pattern] if tar_pattern else 0\n",
    "            bz2_files = open(os.path.join('data', year, month, tar_file), \"r\").read().split(\"\\n\")\n",
    "            bz2_patterns[year][month][tar_file] = {}\n",
    "            for bz2_file in bz2_files:\n",
    "                pattern, values = match_pattern(bz2_file, regex_bz2_patterns)\n",
    "                pattern = regex_bz2_patterns_encrypt[pattern] if pattern else 0\n",
    "                if pattern not in bz2_patterns[year][month][tar_file]:\n",
    "                    bz2_patterns[year][month][tar_file][pattern] = [values]\n",
    "                else:\n",
    "                    bz2_patterns[year][month][tar_file][pattern].append(values)\n",
    "                # if(not check_match(tar_values, values)):\n",
    "                #     print(year, month, tar_file, bz2_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar_patterns = {}\n",
    "\n",
    "for year in data.keys():\n",
    "    tar_patterns[year] = {}\n",
    "    for month in data[year].keys():\n",
    "        tar_patterns[year][month] = {}\n",
    "        for tar_file in data[year][month]:\n",
    "            pattern, values = match_pattern(tar_file, regex_tar_patterns)\n",
    "            pattern = regex_tar_patterns_encrypt[pattern] if pattern else 0\n",
    "            if pattern not in tar_patterns[year][month]:\n",
    "                tar_patterns[year][month][pattern] = [values]\n",
    "            else:\n",
    "                tar_patterns[year][month][pattern].append(values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_months = {}\n",
    "\n",
    "for year in tar_patterns.keys():\n",
    "    for month in tar_patterns[year].keys():\n",
    "        for tar_file in tar_patterns[year][month]:\n",
    "            for values in tar_patterns[year][month][tar_file]:\n",
    "                values_expanded = values if len(values) == 3 else (values[0], values[1], \"00\")\n",
    "                if (year != values[0] or month != values[1]):\n",
    "                    value_to_add = [tar_file, year, month]\n",
    "                else:\n",
    "                    value_to_add = [tar_file]\n",
    "                if(values_expanded not in year_months):\n",
    "                    year_months[values_expanded] = [value_to_add]\n",
    "                else:\n",
    "                    year_months[values_expanded].append(value_to_add)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "year_months_encrypt = {}\n",
    "\n",
    "for values in year_months:\n",
    "    if(values[0] not in year_months_encrypt):\n",
    "        year_months_encrypt[values[0]] = {}\n",
    "    if(values[1] not in year_months_encrypt[values[0]]):\n",
    "        year_months_encrypt[values[0]][values[1]] = {}\n",
    "    \n",
    "    year_months_encrypt[values[0]][values[1]][values[2]] = []\n",
    "\n",
    "    for pattern in year_months[values]:\n",
    "        if(len(pattern) == 3):\n",
    "            year_folder = pattern[0]\n",
    "            month_folder = pattern[1]\n",
    "            tar_file = pattern[2]\n",
    "        elif(len(pattern) == 1):\n",
    "            year_folder = values[0]\n",
    "            month_folder = values[1]\n",
    "            tar_file = pattern[0]\n",
    "        tar_file = regex_tar_patterns_decrypt_str[tar_file].format(*values)\n",
    "        \n",
    "        bz2_files = open(os.path.join('data', year_folder, month_folder, tar_file), \"r\").read().split(\"\\n\")\n",
    "        bz2_pattern_list = list(dict.fromkeys([match_pattern(bz2_file, regex_bz2_patterns)[0] for bz2_file in bz2_files]))\n",
    "        bz2_pattern_list = [(regex_bz2_patterns_encrypt[pattern] if pattern else 0) for pattern in bz2_pattern_list]\n",
    "        year_months_encrypt[values[0]][values[1]][values[2]].append([pattern, bz2_pattern_list])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39184"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "open('year_month_encrypt.json', 'w').write(json.dumps(year_months_encrypt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'01': {'pattern': [[8]], 'bz2_patterns': [1]},\n",
       " '02': {'pattern': [[8]], 'bz2_patterns': [1]},\n",
       " '03': {'pattern': [[8]], 'bz2_patterns': [1]},\n",
       " '04': {'pattern': [[8]], 'bz2_patterns': [1]},\n",
       " '05': {'pattern': [[8]], 'bz2_patterns': [1]},\n",
       " '06': {'pattern': [[8]], 'bz2_patterns': [1]},\n",
       " '07': {'pattern': [[8]], 'bz2_patterns': [1]},\n",
       " '08': {'pattern': [[8]], 'bz2_patterns': [1]},\n",
       " '09': {'pattern': [[8]], 'bz2_patterns': [1]},\n",
       " '10': {'pattern': [[8]], 'bz2_patterns': [1]},\n",
       " '11': {'pattern': [[8]], 'bz2_patterns': [1]},\n",
       " '12': {'pattern': [[8]], 'bz2_patterns': [1]},\n",
       " '13': {'pattern': [[8]], 'bz2_patterns': [1]},\n",
       " '14': {'pattern': [[8]], 'bz2_patterns': [1]},\n",
       " '15': {'pattern': [[8]], 'bz2_patterns': [1]},\n",
       " '16': {'pattern': [[8]], 'bz2_patterns': [1]},\n",
       " '17': {'pattern': [[8]], 'bz2_patterns': [1]},\n",
       " '18': {'pattern': [[8]], 'bz2_patterns': [1]},\n",
       " '19': {'pattern': [[8]], 'bz2_patterns': [1]},\n",
       " '20': {'pattern': [[8]], 'bz2_patterns': [1]},\n",
       " '21': {'pattern': [[8]], 'bz2_patterns': [1]},\n",
       " '22': {'pattern': [[8]], 'bz2_patterns': [1]},\n",
       " '23': {'pattern': [[9]], 'bz2_patterns': []},\n",
       " '24': {'pattern': [[9]], 'bz2_patterns': []},\n",
       " '25': {'pattern': [[9]], 'bz2_patterns': []},\n",
       " '26': {'pattern': [[9]], 'bz2_patterns': []},\n",
       " '27': {'pattern': [[9]], 'bz2_patterns': []},\n",
       " '28': {'pattern': [[9]], 'bz2_patterns': []},\n",
       " '29': {'pattern': [[9]], 'bz2_patterns': []},\n",
       " '30': {'pattern': [[9]], 'bz2_patterns': []},\n",
       " '31': {'pattern': [[9]], 'bz2_patterns': []}}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "year_months_encrypt['2021']['08']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in tar_patterns:\n",
    "    for month in tar_patterns[year]:\n",
    "        if(len(tar_patterns[year][month]) != 1):\n",
    "            print(year, month, [regex_tar_patterns_decrypt_meaning[i] for i in tar_patterns[year][month].keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
